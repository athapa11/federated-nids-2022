{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "from flowmeter.flowmeter import Flowmeter\n",
    "\n",
    "import urllib.request\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aefawef = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_pcap.pcap.gz'\n",
    "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_dataset.csv.gz'\n",
    "label_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_labels.csv.gz'\n",
    "\n",
    "pcap_path = \"C:/Users/Panda 1.0/OneDrive - Loughborough University/Masters/Thesis/LTSM-Test/Mirai_pcap.pcap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 764137 entries, 0 to 764136\n",
      "Columns: 116 entries, 1 to 0\n",
      "dtypes: float64(115), int64(1)\n",
      "memory usage: 676.3 MB\n",
      "dataframe dataset: \n",
      "              1           2             3          4           5    \\\n",
      "0        1.000000   60.000000  0.000000e+00   1.000000   60.000000   \n",
      "1        1.999983   60.000000  0.000000e+00   1.999990   60.000000   \n",
      "2        1.000000   86.000000  0.000000e+00   1.000000   86.000000   \n",
      "3        1.999272   86.000000  9.094947e-13   1.999563   86.000000   \n",
      "4        1.000000   60.000000  0.000000e+00   1.000000   60.000000   \n",
      "...           ...         ...           ...        ...         ...   \n",
      "764132  18.897961   60.185845  3.594147e+00  51.700968   60.182642   \n",
      "764133  19.775173   60.176447  3.414055e+00  52.499151   60.179163   \n",
      "764134  20.692078   60.167920  3.250494e+00  53.366680   60.175805   \n",
      "764135  21.566383   60.160134  3.101020e+00  54.171935   60.172560   \n",
      "764136   1.089072  117.452795  4.912762e+01   1.331557  114.114816   \n",
      "\n",
      "                 6           7           8             9            10   ...  \\\n",
      "0       0.000000e+00    1.000000   60.000000  0.000000e+00     1.000000  ...   \n",
      "1       9.094947e-13    1.999997   60.000000  4.547474e-13     2.000000  ...   \n",
      "2       0.000000e+00    1.000000   86.000000  0.000000e+00     1.000000  ...   \n",
      "3       0.000000e+00    1.999854   86.000000  9.094947e-13     1.999985  ...   \n",
      "4       0.000000e+00    1.000000   60.000000  0.000000e+00     1.000000  ...   \n",
      "...              ...         ...         ...           ...          ...  ...   \n",
      "764132  3.492246e+00  257.391336   60.137137  2.455690e+00  3083.411177  ...   \n",
      "764133  3.426350e+00  258.055986   60.136606  2.446246e+00  3084.009210  ...   \n",
      "764134  3.362736e+00  258.838753   60.136078  2.436867e+00  3084.749498  ...   \n",
      "764135  3.301221e+00  259.523519   60.135554  2.427549e+00  3085.373607  ...   \n",
      "764136  1.394927e+02    2.837263  106.624655  9.384487e+02    27.852158  ...   \n",
      "\n",
      "        107  108           109    110       111         112           113  \\\n",
      "0       0.0  0.0      1.000000   60.0  0.000000   60.000000  0.000000e+00   \n",
      "1       0.0  0.0      1.000000   60.0  0.000000   60.000000  0.000000e+00   \n",
      "2       0.0  0.0      1.000000   86.0  0.000000   86.000000  0.000000e+00   \n",
      "3       0.0  0.0      1.000000   86.0  0.000000   86.000000  0.000000e+00   \n",
      "4       0.0  0.0      1.000000   60.0  0.000000   60.000000  0.000000e+00   \n",
      "...     ...  ...           ...    ...       ...         ...           ...   \n",
      "764132  0.0  0.0  30682.136500   60.0  0.000004   60.000000  1.546141e-11   \n",
      "764133  0.0  0.0  30682.736491   60.0  0.000004   60.000000  1.500666e-11   \n",
      "764134  0.0  0.0  30683.478094   60.0  0.000004   60.000000  1.546141e-11   \n",
      "764135  0.0  0.0  30684.104181   60.0  0.000004   60.000000  1.591616e-11   \n",
      "764136  0.0  0.0      1.000000  119.0  0.000000  166.883193  0.000000e+00   \n",
      "\n",
      "        114  115  0    \n",
      "0       0.0  0.0    0  \n",
      "1       0.0  0.0    0  \n",
      "2       0.0  0.0    0  \n",
      "3       0.0  0.0    0  \n",
      "4       0.0  0.0    0  \n",
      "...     ...  ...  ...  \n",
      "764132  0.0  0.0    1  \n",
      "764133  0.0  0.0    1  \n",
      "764134  0.0  0.0    1  \n",
      "764135  0.0  0.0    1  \n",
      "764136  0.0  0.0    1  \n",
      "\n",
      "[764137 rows x 116 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_path, header=None)\n",
    "df = df.drop(columns=[df.columns[0]])\n",
    "\n",
    "# dataset info:\n",
    "# df.info()\n",
    "# df.head()\n",
    "# print(df.head())\n",
    "\n",
    "# check for missing data (not working atm -check why tmr)\n",
    "# df.nunique() # return number of unique elements per column (excludes NaNs)\n",
    "# df.isnull().sum()\n",
    "# df.describe()\n",
    "\n",
    "\n",
    "labels = pd.read_csv(label_path, dtype={\"\": int, \"x\": 'float64'}, header=None)\n",
    "# labels.info()\n",
    "# labels.head()\n",
    "# labels.describe()\n",
    "# print(labels.head())\n",
    "\n",
    "# append label to the end of dataset:\n",
    "column_wanted = labels.iloc[:, 0]\n",
    "df_dataset = pd.concat([df, column_wanted], axis=1, ignore_index=False)\n",
    "df_dataset = df_dataset.reset_index(drop=True)\n",
    "\n",
    "df_dataset.info()\n",
    "df_dataset.head()\n",
    "df_dataset.describe()\n",
    "print(\"dataframe dataset: \")\n",
    "print(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.          60.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  1.99998265  60.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  1.          86.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " ...\n",
      " [ 20.69207825  60.16791977   3.25049354 ...   0.           0.\n",
      "    1.        ]\n",
      " [ 21.56638256  60.16013358   3.10101998 ...   0.           0.\n",
      "    1.        ]\n",
      " [  1.08907225 117.4527954   49.12761856 ...   0.           0.\n",
      "    1.        ]]\n"
     ]
    }
   ],
   "source": [
    "np_dataset = df_dataset.values\n",
    "print(np_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes cross-silo FL: dataset is split into 10 clients (organisations) to simulate this\n",
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training set and testing set with the labels seperate\n",
    "split_point = int(len(np_dataset)*0.9)\n",
    "\n",
    "train_np = np_dataset[:split_point, :]\n",
    "test_np = np_dataset[split_point:, :]\n",
    "# train_labels = labelled_dataset[:split_point, -1]\n",
    "# test_labels = labelled_dataset[split_point:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KitsuneDataset(Dataset):\n",
    "    def __init__(self, numpy_array):\n",
    "        self.features = numpy_array[:, :-1]\n",
    "        self.labels = numpy_array[:, -1:].ravel()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = torch.tensor(self.features[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets loaded\n",
      "trainloader:  <torch.utils.data.dataloader.DataLoader object at 0x0000024C07B853D0>\n"
     ]
    }
   ],
   "source": [
    "def load_datasets():#num_clients: int):\n",
    "    train_set = KitsuneDataset(train_np)\n",
    "    test_set = KitsuneDataset(test_np)\n",
    "    # # Split training set into partitions(i.e., the no. of clients) to simulate the individual dataset\n",
    "    # partition_size = len(train_tensor) // NUM_CLIENTS\n",
    "    # lengths = [partition_size] * NUM_CLIENTS\n",
    "    # datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # # Split each partition into train/val and create DataLoader\n",
    "    # trainloaders = []\n",
    "    # valloaders = []\n",
    "\n",
    "    # for ds in datasets:\n",
    "\n",
    "    #     trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=False))\n",
    "    #     valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False))\n",
    "\n",
    "    # w/o partitioning for clients:\n",
    "\n",
    "    # create validation subset from training set:\n",
    "    train_size = int(len(train_set) * 0.8)\n",
    "    train_split = train_set[:train_size], \n",
    "    val_split = train_set[train_size:]\n",
    "\n",
    "    # create dataloaders\n",
    "    trainloader = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    valloader = DataLoader(val_split, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    testloader = DataLoader(test_np, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "# load datasets\n",
    "trainloader, valloader, testloader = load_datasets()\n",
    "print(\"datasets loaded\")\n",
    "print(\"trainloader: \", trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 550178, 115])\n",
      "torch.Size([1, 550178])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in trainloader:\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=0.0, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: input tensor of shape (batch_size, seq_len, input_size)\n",
    "        print(x.size())\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # hidden: tuple of hidden state and cell state tensors from previous time step\n",
    "        # intially set to all zeros\n",
    "        hidden = (torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size),\n",
    "                  torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size))\n",
    "        \n",
    "        # output: output tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        # Get the last output in the sequence and pass it through a fully connected layer\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, epochs: int, verbose=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    model.train() # switch to train mode\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            labels = labels.unsqueeze(1)\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad() # zero parameter gradients\n",
    "\n",
    "            # reset hidden state\n",
    "            hidden = (torch.zeros(model.lstm.num_layers, inputs.size(0), model.lstm.hidden_size),\n",
    "                      torch.zeros(model.lstm.num_layers, inputs.size(0), model.lstm.hidden_size))\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss # update loss\n",
    "\n",
    "            # compute accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        \n",
    "        # log results of training\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, testloader):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            labels.unsqueeze(1)\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # reset hidden state\n",
    "            hidden = (torch.zeros(model.lstm.num_layers, inputs.size(0), model.lstm.hidden_size).to(device),\n",
    "                      torch.zeros(model.lstm.num_layers, inputs.size(0), model.lstm.hidden_size).to(device))\n",
    "\n",
    "            # forward pass + loss update\n",
    "            outputs = model(inputs)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    # print test results\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 1.13.1+cu116 and Flower 1.3.0\n",
      "model initialised\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")\n",
    "\n",
    "\n",
    "# centralised training:\n",
    "# trainloader = trainloaders[0]\n",
    "# valloader = valloaders[0]\n",
    "model = LSTM(115, 10, 2, 1).to(DEVICE)\n",
    "print(\"model initialised\")\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     train(model, trainloader, 1)\n",
    "#     print(\"training occured\")\n",
    "#     loss, accuracy = test(model, valloader)\n",
    "#     print(\"testing occured\")\n",
    "#     print(f\"Epoch {epoch+1}: validation loss {loss}, accuracy {accuracy}\")\n",
    "\n",
    "# loss, accuracy = test(model, testloader)\n",
    "# print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
